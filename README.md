# Pretrained Models Final Project SoSe2024
##Â Exploring Cross-Lingual Subtitle Segmentation with Multilingual BERT Models

## Description
Small independent project for Pretrained Models (SoSe24), MSc Cognitive Systems.
The project evaluates the cross-lingual effectiveness of a fine-tuned multilingual BERT model and an XLM-Roberta model on a the task of subtitle segmentation, with the aim of developing a multilingual model capable of generating well-formed subtitles in various languages. After fine-tuning both models with an English dataset of labelled subtitles, both models were able to produce syntactically-aware line splits when tested on unsegmented subtitles in English and Spanish.

## Author
Emma Carballal Haire

Email: emma.carballal@uni-potsdam.de